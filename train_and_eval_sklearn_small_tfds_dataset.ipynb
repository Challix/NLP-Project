{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q59kntpEjeax"
   },
   "source": [
    "##### Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fhjU0f_ja9h"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RPI9UwajiPI"
   },
   "source": [
    "##### Full-flow evaluation\n",
    "\n",
    "A colab for testing the full flow of calculating embeddings and train/eval using sklearn models. Since this notebook doesn't parallelize (like the apache beam tools do) and computing embeddings is computationally expensive, **please use the mutli-step beam-based tools if** you'd like to eval a large dataset, eval a custom dataset, or train a Keras model.\n",
    "\n",
    "Please be sure to use a **Python 3** kernel. **Running on GPU** significantly speeds up the process as well.\n",
    "\n",
    "Conceptual overview of this colab:\n",
    "\n",
    "1. Read `TensorFlow Dataset` data as numpy\n",
    "1. Convert audio to float and resample\n",
    "1. Convert audio to embeddings\n",
    "1. Train and eval sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKUuSLehjke7"
   },
   "outputs": [],
   "source": [
    "tfds_dataset_name = 'crema_d'  #@param\n",
    "REQUIRED_SAMPLE_RATE_ = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQvwTuBjjnRZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 10:14:37.272147: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 579.25 MiB (download: 579.25 MiB, generated: 1.65 GiB, total: 2.21 GiB) to /Users/morgan/tensorflow_datasets/crema_d/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961bebb07d8a4f4eabdb1d5e209b13d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17af39abda4493aaaac6b875356acde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a3739c8d4482c963b67881f21f1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa79f60af1a4604a29682e109fc8312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9956756f12dc47869b2c728d0d6c6ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc87cd9302744f3b746cb67e2e87b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/5144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/morgan/tensorflow_datasets/crema_d/1.0.0.incomplete3W380S/crema_d-train.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/738 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/morgan/tensorflow_datasets/crema_d/1.0.0.incomplete3W380S/crema_d-validation.tfrecord*...:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/1556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/morgan/tensorflow_datasets/crema_d/1.0.0.incomplete3W380S/crema_d-test.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset crema_d downloaded and prepared to /Users/morgan/tensorflow_datasets/crema_d/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 10:17:02.987654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished train\n",
      "Finished validation\n",
      "Finished test\n"
     ]
    }
   ],
   "source": [
    "# Read the data into numpy arrays.\n",
    "import collections\n",
    "SingleSplit = collections.namedtuple(\n",
    "    'SingleSplit', ['audio', 'labels', 'speaker_id'])\n",
    "Data = collections.namedtuple(\n",
    "    'Data', ['train', 'validation', 'test'])\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "assert tf.executing_eagerly()\n",
    "import tensorflow_datasets as tfds\n",
    "def _dat_from_split(split):\n",
    "  np_generator = tfds.as_numpy(tfds.load(tfds_dataset_name, split=split))\n",
    "  dat = [(x['audio'], x['label'], x['speaker_id']) for x in np_generator]\n",
    "  audio, labels, speaker_id = zip(*dat)\n",
    "\n",
    "  import numpy as np\n",
    "  labels = np.array(labels, dtype=np.int16)\n",
    "  speaker_id = np.array(speaker_id)\n",
    "  assert len(audio) == labels.size == speaker_id.size\n",
    "  assert labels.ndim == speaker_id.ndim == 1\n",
    "  print(f'Finished {split}')\n",
    "  return audio, labels, speaker_id\n",
    "\n",
    "all_data = Data(\n",
    "    train=SingleSplit(*_dat_from_split('train')),\n",
    "    validation=SingleSplit(*_dat_from_split('validation')),\n",
    "    test=SingleSplit(*_dat_from_split('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhL0uSMjjpQN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished resampling 0 / 5144 for train\n",
      "Finished resampling 50 / 5144 for train\n",
      "Finished resampling 100 / 5144 for train\n",
      "Finished resampling 150 / 5144 for train\n",
      "Finished resampling 200 / 5144 for train\n",
      "Finished resampling 250 / 5144 for train\n",
      "Finished resampling 300 / 5144 for train\n",
      "Finished resampling 350 / 5144 for train\n",
      "Finished resampling 400 / 5144 for train\n",
      "Finished resampling 450 / 5144 for train\n",
      "Finished resampling 500 / 5144 for train\n",
      "Finished resampling 550 / 5144 for train\n",
      "Finished resampling 600 / 5144 for train\n",
      "Finished resampling 650 / 5144 for train\n",
      "Finished resampling 700 / 5144 for train\n",
      "Finished resampling 750 / 5144 for train\n",
      "Finished resampling 800 / 5144 for train\n",
      "Finished resampling 850 / 5144 for train\n",
      "Finished resampling 900 / 5144 for train\n",
      "Finished resampling 950 / 5144 for train\n",
      "Finished resampling 1000 / 5144 for train\n",
      "Finished resampling 1050 / 5144 for train\n",
      "Finished resampling 1100 / 5144 for train\n",
      "Finished resampling 1150 / 5144 for train\n",
      "Finished resampling 1200 / 5144 for train\n",
      "Finished resampling 1250 / 5144 for train\n",
      "Finished resampling 1300 / 5144 for train\n",
      "Finished resampling 1350 / 5144 for train\n",
      "Finished resampling 1400 / 5144 for train\n",
      "Finished resampling 1450 / 5144 for train\n",
      "Finished resampling 1500 / 5144 for train\n",
      "Finished resampling 1550 / 5144 for train\n",
      "Finished resampling 1600 / 5144 for train\n",
      "Finished resampling 1650 / 5144 for train\n",
      "Finished resampling 1700 / 5144 for train\n",
      "Finished resampling 1750 / 5144 for train\n",
      "Finished resampling 1800 / 5144 for train\n",
      "Finished resampling 1850 / 5144 for train\n",
      "Finished resampling 1900 / 5144 for train\n",
      "Finished resampling 1950 / 5144 for train\n",
      "Finished resampling 2000 / 5144 for train\n",
      "Finished resampling 2050 / 5144 for train\n",
      "Finished resampling 2100 / 5144 for train\n",
      "Finished resampling 2150 / 5144 for train\n",
      "Finished resampling 2200 / 5144 for train\n",
      "Finished resampling 2250 / 5144 for train\n",
      "Finished resampling 2300 / 5144 for train\n",
      "Finished resampling 2350 / 5144 for train\n",
      "Finished resampling 2400 / 5144 for train\n",
      "Finished resampling 2450 / 5144 for train\n",
      "Finished resampling 2500 / 5144 for train\n",
      "Finished resampling 2550 / 5144 for train\n",
      "Finished resampling 2600 / 5144 for train\n",
      "Finished resampling 2650 / 5144 for train\n",
      "Finished resampling 2700 / 5144 for train\n",
      "Finished resampling 2750 / 5144 for train\n",
      "Finished resampling 2800 / 5144 for train\n",
      "Finished resampling 2850 / 5144 for train\n",
      "Finished resampling 2900 / 5144 for train\n",
      "Finished resampling 2950 / 5144 for train\n",
      "Finished resampling 3000 / 5144 for train\n",
      "Finished resampling 3050 / 5144 for train\n",
      "Finished resampling 3100 / 5144 for train\n",
      "Finished resampling 3150 / 5144 for train\n",
      "Finished resampling 3200 / 5144 for train\n",
      "Finished resampling 3250 / 5144 for train\n",
      "Finished resampling 3300 / 5144 for train\n",
      "Finished resampling 3350 / 5144 for train\n",
      "Finished resampling 3400 / 5144 for train\n",
      "Finished resampling 3450 / 5144 for train\n",
      "Finished resampling 3500 / 5144 for train\n",
      "Finished resampling 3550 / 5144 for train\n",
      "Finished resampling 3600 / 5144 for train\n",
      "Finished resampling 3650 / 5144 for train\n",
      "Finished resampling 3700 / 5144 for train\n",
      "Finished resampling 3750 / 5144 for train\n",
      "Finished resampling 3800 / 5144 for train\n",
      "Finished resampling 3850 / 5144 for train\n",
      "Finished resampling 3900 / 5144 for train\n",
      "Finished resampling 3950 / 5144 for train\n",
      "Finished resampling 4000 / 5144 for train\n",
      "Finished resampling 4050 / 5144 for train\n",
      "Finished resampling 4100 / 5144 for train\n",
      "Finished resampling 4150 / 5144 for train\n",
      "Finished resampling 4200 / 5144 for train\n",
      "Finished resampling 4250 / 5144 for train\n",
      "Finished resampling 4300 / 5144 for train\n",
      "Finished resampling 4350 / 5144 for train\n",
      "Finished resampling 4400 / 5144 for train\n",
      "Finished resampling 4450 / 5144 for train\n",
      "Finished resampling 4500 / 5144 for train\n",
      "Finished resampling 4550 / 5144 for train\n",
      "Finished resampling 4600 / 5144 for train\n",
      "Finished resampling 4650 / 5144 for train\n",
      "Finished resampling 4700 / 5144 for train\n",
      "Finished resampling 4750 / 5144 for train\n",
      "Finished resampling 4800 / 5144 for train\n",
      "Finished resampling 4850 / 5144 for train\n",
      "Finished resampling 4900 / 5144 for train\n",
      "Finished resampling 4950 / 5144 for train\n",
      "Finished resampling 5000 / 5144 for train\n",
      "Finished resampling 5050 / 5144 for train\n",
      "Finished resampling 5100 / 5144 for train\n",
      "Finished train\n",
      "Finished resampling 0 / 738 for validation\n",
      "Finished resampling 50 / 738 for validation\n",
      "Finished resampling 100 / 738 for validation\n",
      "Finished resampling 150 / 738 for validation\n",
      "Finished resampling 200 / 738 for validation\n",
      "Finished resampling 250 / 738 for validation\n",
      "Finished resampling 300 / 738 for validation\n",
      "Finished resampling 350 / 738 for validation\n",
      "Finished resampling 400 / 738 for validation\n",
      "Finished resampling 450 / 738 for validation\n",
      "Finished resampling 500 / 738 for validation\n",
      "Finished resampling 550 / 738 for validation\n",
      "Finished resampling 600 / 738 for validation\n",
      "Finished resampling 650 / 738 for validation\n",
      "Finished resampling 700 / 738 for validation\n",
      "Finished validation\n",
      "Finished resampling 0 / 1556 for test\n",
      "Finished resampling 50 / 1556 for test\n",
      "Finished resampling 100 / 1556 for test\n",
      "Finished resampling 150 / 1556 for test\n",
      "Finished resampling 200 / 1556 for test\n",
      "Finished resampling 250 / 1556 for test\n",
      "Finished resampling 300 / 1556 for test\n",
      "Finished resampling 350 / 1556 for test\n",
      "Finished resampling 400 / 1556 for test\n",
      "Finished resampling 450 / 1556 for test\n",
      "Finished resampling 500 / 1556 for test\n",
      "Finished resampling 550 / 1556 for test\n",
      "Finished resampling 600 / 1556 for test\n",
      "Finished resampling 650 / 1556 for test\n",
      "Finished resampling 700 / 1556 for test\n",
      "Finished resampling 750 / 1556 for test\n",
      "Finished resampling 800 / 1556 for test\n",
      "Finished resampling 850 / 1556 for test\n",
      "Finished resampling 900 / 1556 for test\n",
      "Finished resampling 950 / 1556 for test\n",
      "Finished resampling 1000 / 1556 for test\n",
      "Finished resampling 1050 / 1556 for test\n",
      "Finished resampling 1100 / 1556 for test\n",
      "Finished resampling 1150 / 1556 for test\n",
      "Finished resampling 1200 / 1556 for test\n",
      "Finished resampling 1250 / 1556 for test\n",
      "Finished resampling 1300 / 1556 for test\n",
      "Finished resampling 1350 / 1556 for test\n",
      "Finished resampling 1400 / 1556 for test\n",
      "Finished resampling 1450 / 1556 for test\n",
      "Finished resampling 1500 / 1556 for test\n",
      "Finished resampling 1550 / 1556 for test\n",
      "Finished test\n"
     ]
    }
   ],
   "source": [
    "# Make the audio floats, and resample the audio if necessary.\n",
    "import collections\n",
    "import librosa\n",
    "import numpy as np\n",
    "FloatData = collections.namedtuple('FloatData', ['train', 'validation', 'test'])\n",
    "\n",
    "sample_rate = tfds.builder(tfds_dataset_name).info.features['audio'].sample_rate\n",
    "def _int_to_float(audio_int16, split_name):\n",
    "  float_audio_16k = []\n",
    "  for i, samples in enumerate(audio_int16):\n",
    "    float_audio = samples.astype(np.float32) / np.iinfo(np.int16).max\n",
    "    if sample_rate != REQUIRED_SAMPLE_RATE_:\n",
    "      float_audio = librosa.core.resample(\n",
    "          float_audio, orig_sr=sample_rate, target_sr=16000, \n",
    "          res_type='kaiser_best')\n",
    "    float_audio_16k.append(float_audio)\n",
    "    if i % 50 == 0:\n",
    "      print(f'Finished resampling {i} / {len(audio_int16)} for {split_name}')\n",
    "  print(f'Finished {split_name}')\n",
    "  return float_audio_16k\n",
    "\n",
    "\n",
    "float_audio_16k = FloatData(\n",
    "    train=_int_to_float(all_data.train.audio, 'train'),\n",
    "    validation=_int_to_float(all_data.validation.audio, 'validation'),\n",
    "    test=_int_to_float(all_data.test.audio, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIb4stZ3jqsF"
   },
   "outputs": [],
   "source": [
    "tfhub_model_name = 'https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/1'  #@param\n",
    "output_key = 'embedding'  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SAsuvE5jrsN"
   },
   "outputs": [],
   "source": [
    "# Convert the audio to embeddings. Preaverage the embeddings across time.\n",
    "import tensorflow_hub as hub\n",
    "model = hub.load(tfhub_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UtmnSGPjt1k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding 0 / 5144 for train\n",
      "Finished embedding 50 / 5144 for train\n",
      "Finished embedding 100 / 5144 for train\n",
      "Finished embedding 150 / 5144 for train\n",
      "Finished embedding 200 / 5144 for train\n",
      "Finished embedding 250 / 5144 for train\n",
      "Finished embedding 300 / 5144 for train\n",
      "Finished embedding 350 / 5144 for train\n",
      "Finished embedding 400 / 5144 for train\n",
      "Finished embedding 450 / 5144 for train\n",
      "Finished embedding 500 / 5144 for train\n",
      "Finished embedding 550 / 5144 for train\n",
      "Finished embedding 600 / 5144 for train\n",
      "Finished embedding 650 / 5144 for train\n",
      "Finished embedding 700 / 5144 for train\n",
      "Finished embedding 750 / 5144 for train\n",
      "Finished embedding 800 / 5144 for train\n",
      "Finished embedding 850 / 5144 for train\n",
      "Finished embedding 900 / 5144 for train\n",
      "Finished embedding 950 / 5144 for train\n",
      "Finished embedding 1000 / 5144 for train\n",
      "Finished embedding 1050 / 5144 for train\n",
      "Finished embedding 1100 / 5144 for train\n",
      "Finished embedding 1150 / 5144 for train\n",
      "Finished embedding 1200 / 5144 for train\n",
      "Finished embedding 1250 / 5144 for train\n",
      "Finished embedding 1300 / 5144 for train\n",
      "Finished embedding 1350 / 5144 for train\n",
      "Finished embedding 1400 / 5144 for train\n",
      "Finished embedding 1450 / 5144 for train\n",
      "Finished embedding 1500 / 5144 for train\n",
      "Finished embedding 1550 / 5144 for train\n",
      "Finished embedding 1600 / 5144 for train\n",
      "Finished embedding 1650 / 5144 for train\n",
      "Finished embedding 1700 / 5144 for train\n",
      "Finished embedding 1750 / 5144 for train\n",
      "Finished embedding 1800 / 5144 for train\n",
      "Finished embedding 1850 / 5144 for train\n",
      "Finished embedding 1900 / 5144 for train\n",
      "Finished embedding 1950 / 5144 for train\n",
      "Finished embedding 2000 / 5144 for train\n",
      "Finished embedding 2050 / 5144 for train\n",
      "Finished embedding 2100 / 5144 for train\n",
      "Finished embedding 2150 / 5144 for train\n",
      "Finished embedding 2200 / 5144 for train\n",
      "Finished embedding 2250 / 5144 for train\n",
      "Finished embedding 2300 / 5144 for train\n",
      "Finished embedding 2350 / 5144 for train\n",
      "Finished embedding 2400 / 5144 for train\n",
      "Finished embedding 2450 / 5144 for train\n",
      "Finished embedding 2500 / 5144 for train\n",
      "Finished embedding 2550 / 5144 for train\n",
      "Finished embedding 2600 / 5144 for train\n",
      "Finished embedding 2650 / 5144 for train\n",
      "Finished embedding 2700 / 5144 for train\n",
      "Finished embedding 2750 / 5144 for train\n",
      "Finished embedding 2800 / 5144 for train\n",
      "Finished embedding 2850 / 5144 for train\n",
      "Finished embedding 2900 / 5144 for train\n",
      "Finished embedding 2950 / 5144 for train\n",
      "Finished embedding 3000 / 5144 for train\n",
      "Finished embedding 3050 / 5144 for train\n",
      "Finished embedding 3100 / 5144 for train\n",
      "Finished embedding 3150 / 5144 for train\n",
      "Finished embedding 3200 / 5144 for train\n",
      "Finished embedding 3250 / 5144 for train\n",
      "Finished embedding 3300 / 5144 for train\n",
      "Finished embedding 3350 / 5144 for train\n",
      "Finished embedding 3400 / 5144 for train\n",
      "Finished embedding 3450 / 5144 for train\n",
      "Finished embedding 3500 / 5144 for train\n",
      "Finished embedding 3550 / 5144 for train\n",
      "Finished embedding 3600 / 5144 for train\n",
      "Finished embedding 3650 / 5144 for train\n",
      "Finished embedding 3700 / 5144 for train\n",
      "Finished embedding 3750 / 5144 for train\n",
      "Finished embedding 3800 / 5144 for train\n",
      "Finished embedding 3850 / 5144 for train\n",
      "Finished embedding 3900 / 5144 for train\n",
      "Finished embedding 3950 / 5144 for train\n",
      "Finished embedding 4000 / 5144 for train\n",
      "Finished embedding 4050 / 5144 for train\n",
      "Finished embedding 4100 / 5144 for train\n",
      "Finished embedding 4150 / 5144 for train\n",
      "Finished embedding 4200 / 5144 for train\n",
      "Finished embedding 4250 / 5144 for train\n",
      "Finished embedding 4300 / 5144 for train\n",
      "Finished embedding 4350 / 5144 for train\n",
      "Finished embedding 4400 / 5144 for train\n",
      "Finished embedding 4450 / 5144 for train\n",
      "Finished embedding 4500 / 5144 for train\n",
      "Finished embedding 4550 / 5144 for train\n",
      "Finished embedding 4600 / 5144 for train\n",
      "Finished embedding 4650 / 5144 for train\n",
      "Finished embedding 4700 / 5144 for train\n",
      "Finished embedding 4750 / 5144 for train\n",
      "Finished embedding 4800 / 5144 for train\n",
      "Finished embedding 4850 / 5144 for train\n",
      "Finished embedding 4900 / 5144 for train\n",
      "Finished embedding 4950 / 5144 for train\n",
      "Finished embedding 5000 / 5144 for train\n",
      "Finished embedding 5050 / 5144 for train\n",
      "Finished embedding 5100 / 5144 for train\n",
      "Finished train\n",
      "Finished embedding 0 / 738 for validation\n",
      "Finished embedding 50 / 738 for validation\n",
      "Finished embedding 100 / 738 for validation\n",
      "Finished embedding 150 / 738 for validation\n",
      "Finished embedding 200 / 738 for validation\n",
      "Finished embedding 250 / 738 for validation\n",
      "Finished embedding 300 / 738 for validation\n",
      "Finished embedding 350 / 738 for validation\n",
      "Finished embedding 400 / 738 for validation\n",
      "Finished embedding 450 / 738 for validation\n",
      "Finished embedding 500 / 738 for validation\n",
      "Finished embedding 550 / 738 for validation\n",
      "Finished embedding 600 / 738 for validation\n",
      "Finished embedding 650 / 738 for validation\n",
      "Finished embedding 700 / 738 for validation\n",
      "Finished validation\n",
      "Finished embedding 0 / 1556 for test\n",
      "Finished embedding 50 / 1556 for test\n",
      "Finished embedding 100 / 1556 for test\n",
      "Finished embedding 150 / 1556 for test\n",
      "Finished embedding 200 / 1556 for test\n",
      "Finished embedding 250 / 1556 for test\n",
      "Finished embedding 300 / 1556 for test\n",
      "Finished embedding 350 / 1556 for test\n",
      "Finished embedding 400 / 1556 for test\n",
      "Finished embedding 450 / 1556 for test\n",
      "Finished embedding 500 / 1556 for test\n",
      "Finished embedding 550 / 1556 for test\n",
      "Finished embedding 600 / 1556 for test\n",
      "Finished embedding 650 / 1556 for test\n",
      "Finished embedding 700 / 1556 for test\n",
      "Finished embedding 750 / 1556 for test\n",
      "Finished embedding 800 / 1556 for test\n",
      "Finished embedding 850 / 1556 for test\n",
      "Finished embedding 900 / 1556 for test\n",
      "Finished embedding 950 / 1556 for test\n",
      "Finished embedding 1000 / 1556 for test\n",
      "Finished embedding 1050 / 1556 for test\n",
      "Finished embedding 1100 / 1556 for test\n",
      "Finished embedding 1150 / 1556 for test\n",
      "Finished embedding 1200 / 1556 for test\n",
      "Finished embedding 1250 / 1556 for test\n",
      "Finished embedding 1300 / 1556 for test\n",
      "Finished embedding 1350 / 1556 for test\n",
      "Finished embedding 1400 / 1556 for test\n",
      "Finished embedding 1450 / 1556 for test\n",
      "Finished embedding 1500 / 1556 for test\n",
      "Finished embedding 1550 / 1556 for test\n",
      "Finished test\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "Embeddings = collections.namedtuple(\n",
    "    'Embeddings', ['train', 'validation', 'test'])\n",
    "\n",
    "def _calc_embeddings(cur_float_audio, split_name):\n",
    "  cur_embeddings = []\n",
    "  for i, float_samples in enumerate(cur_float_audio):\n",
    "    tf_out = model(tf.constant(float_samples, tf.float32),\n",
    "                  tf.constant(16000, tf.int32))\n",
    "    embedding_2d = tf_out[output_key]\n",
    "    assert embedding_2d.ndim == 2\n",
    "    embedding_1d = np.mean(embedding_2d, axis=0)\n",
    "    cur_embeddings.append(embedding_1d)\n",
    "    if i % 50 == 0:\n",
    "      print(f'Finished embedding {i} / {len(cur_float_audio)} for {split_name}')\n",
    "  print(f'Finished {split_name}')\n",
    "  cur_embeddings = np.array(cur_embeddings, dtype=np.float32)\n",
    "  return cur_embeddings\n",
    "\n",
    "embeddings = Embeddings(\n",
    "    train=_calc_embeddings(float_audio_16k.train, 'train'),\n",
    "    validation=_calc_embeddings(float_audio_16k.validation, 'validation'),\n",
    "    test=_calc_embeddings(float_audio_16k.test, 'test'))\n",
    "assert embeddings.train.shape[1] == embeddings.validation.shape[1] == embeddings.test.shape[1]\n",
    "assert embeddings.train.shape[0] == all_data.train.labels.shape[0] == all_data.train.speaker_id.shape[0]\n",
    "assert embeddings.validation.shape[0] == all_data.validation.labels.shape[0] == all_data.validation.speaker_id.shape[0]\n",
    "assert embeddings.test.shape[0] == all_data.test.labels.shape[0] == all_data.test.speaker_id.shape[0]\n",
    "assert not np.isnan(embeddings.train).any()\n",
    "assert not np.isnan(embeddings.validation).any()\n",
    "assert not np.isnan(embeddings.test).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6BTvIGQjwt3"
   },
   "outputs": [],
   "source": [
    "model_name = 'LogisticRegression_balanced'  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVVFxPrcjyak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_balanced eval score: 0.6355013550135501\n",
      "LogisticRegression_balanced test score: 0.6240359897172236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morgan/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def get_sklearn_model(model_name):\n",
    "  return {\n",
    "      'LogisticRegression': lambda: linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial'),\n",
    "      'LogisticRegression_balanced': lambda: linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', class_weight='balanced'),\n",
    "  }[model_name]()\n",
    "\n",
    "def _speaker_normalization(embedding_np, speaker_id_np):\n",
    "  \"\"\"Normalize embedding features by per-speaker statistics.\"\"\"\n",
    "  all_speaker_ids = np.unique(speaker_id_np)\n",
    "  for speaker in all_speaker_ids:\n",
    "    cur_i = speaker_id_np == speaker\n",
    "    embedding_np[cur_i] -= embedding_np[cur_i].mean(axis=0)\n",
    "    stds = embedding_np[cur_i].std(axis=0)\n",
    "    stds[stds == 0] = 1\n",
    "    embedding_np[cur_i] /= stds\n",
    "\n",
    "  return embedding_np\n",
    "\n",
    "# Train models.\n",
    "d = get_sklearn_model(model_name)\n",
    "normalized_train = _speaker_normalization(\n",
    "    embeddings.train, all_data.train.speaker_id)\n",
    "d.fit(normalized_train, all_data.train.labels)\n",
    "\n",
    "# Eval.\n",
    "normalized_validation = _speaker_normalization(\n",
    "    embeddings.validation, all_data.validation.speaker_id)\n",
    "eval_score = d.score(normalized_validation, all_data.validation.labels)\n",
    "print(f'{model_name} eval score: {eval_score}')\n",
    "\n",
    "# Test.\n",
    "normalized_test = _speaker_normalization(\n",
    "    embeddings.test, all_data.test.speaker_id)\n",
    "test_score = d.score(normalized_test, all_data.test.labels)\n",
    "print(f'{model_name} test score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train and eval sklearn small TFDS dataset",
   "provenance": [
    {
     "file_id": "1UE-jDSsEQ0qRvxtw_aRK2k4dJwytUdSk",
     "timestamp": 1587402478951
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
